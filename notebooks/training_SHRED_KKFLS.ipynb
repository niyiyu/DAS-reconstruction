{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bda9187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys, gc\n",
    "\n",
    "# please update this path accordingly\n",
    "sys.path.append(\"../../DAS-reconstruction/scripts/\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import glob\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import clean_up, count_weights\n",
    "from models import SHRED\n",
    "from datasets import DASDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec93ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please update this path accordingly\n",
    "flist = glob.glob(\"../../datasets/earthquakes/*\")\n",
    "\n",
    "nsample_train = 300\n",
    "nsample_val = 300\n",
    "nsample_test = 300\n",
    "\n",
    "ncha = 151\n",
    "ntime = 200\n",
    "\n",
    "ncha_start = 1000\n",
    "ncha_end = 6000\n",
    "noutput = 1000\n",
    "dcha = int(noutput/ncha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx = np.linspace(1, noutput, ncha, dtype = 'int') - 1\n",
    "\n",
    "X = np.zeros([len(flist)*nsample_train, ntime, ncha])\n",
    "Y = np.zeros([len(flist)*nsample_train, noutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f98ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for fname in tqdm(flist):\n",
    "    f = h5py.File(fname, 'r')\n",
    "    \n",
    "    data = f['/Acquisition/Raw[0]/RawData'][:, ncha_start:ncha_end].T\n",
    "    data -= np.mean(data, axis=-1, keepdims=True)\n",
    "    data /= np.std(data, axis=-1, keepdims=True)\n",
    "    f.close()\n",
    "\n",
    "    for _ in range(nsample_train):\n",
    "        idt = np.random.randint(ntime+1, 3000)                        # last time index\n",
    "        ic  = np.random.randint(0, ncha_end-ncha_start-noutput)       # first channel indexes\n",
    "        X[i, :, :] = data[ic+cidx, idt-(ntime-1):idt+1].T\n",
    "        Y[i, :] = data[ic:ic+noutput, idt]\n",
    "        i += 1\n",
    "print(f\"training set size: {i}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(flist)*nsample_train)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "idx_train = idx[:int(0.7*len(idx))]\n",
    "idx_val = idx[int(0.7*len(idx)):int(0.75*len(idx))]\n",
    "idx_test = idx[int(0.75*len(idx)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = torch.Tensor(X[idx_train, :, :])\n",
    "Y_train_ts = torch.Tensor(Y[idx_train, :])\n",
    "\n",
    "X_val_ts = torch.Tensor(X[idx_val, :, :])\n",
    "Y_val_ts = torch.Tensor(Y[idx_val, :])\n",
    "\n",
    "X_test_ts = torch.Tensor(X[idx_test, :, :])\n",
    "Y_test_ts = torch.Tensor(Y[idx_test, :])\n",
    "\n",
    "dataset = DASDataset(X_train_ts, Y_train_ts)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = DASDataset(X_val_ts, Y_val_ts)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = DASDataset(X_test_ts, Y_test_ts)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "\n",
    "print(\"train: \", X_train_ts.shape, Y_train_ts.shape)\n",
    "print(\"validate: \", X_val_ts.shape, Y_val_ts.shape)\n",
    "print(\"test: \", X_test_ts.shape, Y_test_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhidden = 150\n",
    "nlstm = 2\n",
    "\n",
    "model = SHRED(ncha, nhidden, noutput, nlstm)\n",
    "device = torch.device('cuda')\n",
    "model.to(device);\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "print(f\"The model have total {count_weights(model)} weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "nepoch = 80\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=nepoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acaf0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_log = []; val_loss_log = []; test_loss_log = []\n",
    "\n",
    "t0 = time.time()    \n",
    "for t in range(nepoch):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for batch_id, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        optimizer.zero_grad() # Backpropagation\n",
    "        pred = model(batch[0].to(device))\n",
    "        loss = loss_fn(pred, batch[1].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in tqdm(enumerate(val_data_loader), total=len(val_data_loader)):\n",
    "            pred = model(batch[0].to(device))\n",
    "            loss = loss_fn(pred, batch[1].to(device))\n",
    "            val_loss.append(loss.item())\n",
    "    val_loss_log.append(np.mean(val_loss))\n",
    "    \n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n",
    "            pred = model(batch[0].to(device))\n",
    "            loss = loss_fn(pred, batch[1].to(device))\n",
    "            test_loss.append(loss.item())\n",
    "    test_loss_log.append(np.mean(test_loss))\n",
    "    \n",
    "    train_loss_log.append(np.mean(train_loss))\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: Adam lr %.4f -> %.4f\" % (t, before_lr, after_lr))\n",
    "    print(\"%d, %.4f, %.4f, %.4f\" % (t, np.mean(train_loss), np.mean(test_loss), np.mean(val_loss)))\n",
    "    \n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \n",
    "       f\"../../datasets/weights/SHRED_KKFLS_25Hz_151i_1000o_200sp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f713774",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5), dpi = 300)\n",
    "plt.plot(train_loss_log, \".-\", label = 'training')\n",
    "plt.plot(val_loss_log, \".-\", label = 'validation')\n",
    "plt.plot(test_loss_log, \".-\", label = 'testing')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\", fontsize = 15)\n",
    "plt.ylabel(\"Loss\", fontsize = 15)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"../figures/manuscripts/FigS_loss.pdf\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "idx = np.random.randint(0, len(X_val_ts))\n",
    "inputs = X_val_ts[idx, :, :]\n",
    "label = Y_val_ts[idx, :]\n",
    "predict = model(inputs.to(device)).cpu().detach().numpy()\n",
    "print(torch.mean((label-predict)**2))\n",
    "\n",
    "plt.figure(figsize = (25, 10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(label, linewidth = 2.5)\n",
    "plt.plot(predict, '--', linewidth = 2.5)\n",
    "\n",
    "plt.scatter(cidx, label[cidx], marker='+', color='r', zorder=90, s=400, linewidth=3)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(label - predict, linewidth = 2.5)\n",
    "plt.ylim([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db04a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist2 = glob.glob(f\"../../datasets/earthquakes/*.h5\")\n",
    "fname = flist2[10]\n",
    "f = h5py.File(fname, 'r')\n",
    "data = f['/Acquisition/Raw[0]/RawData'][:, 1000:2000].T\n",
    "starttime = UTCDateTime(dict(f['/Acquisition/'].attrs)['MeasurementStartTime'])\n",
    "f.close()\n",
    "\n",
    "vmax = 3\n",
    "x_max = 2000\n",
    "\n",
    "data -= np.mean(data, axis=-1, keepdims=True)\n",
    "data /= np.std(data, axis=-1, keepdims=True)\n",
    "\n",
    "dout = np.zeros(data.shape)\n",
    "for i in range(ntime, data.shape[1]):\n",
    "    din = torch.Tensor(data[cidx, i-(ntime-1):i+1].copy().T)\n",
    "    dout[:, i] = model(din.to(device)).cpu().detach().numpy()\n",
    "\n",
    "res = dout - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7130542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8), dpi=300)\n",
    "# plt.suptitle(f\"{eid} M{mag}\")\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(data[:500, :], aspect='auto', cmap='RdBu',  origin='lower', \n",
    "           vmax = vmax, vmin = -vmax)\n",
    "plt.title(\"original\", fontsize=20)\n",
    "plt.ylim([0, 500]); plt.xlim([ntime, x_max])\n",
    "plt.xticks([]); plt.yticks([])\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(dout[:500, :], aspect='auto', cmap='RdBu',  origin='lower', \n",
    "           vmax = vmax, vmin = -vmax)\n",
    "plt.title(\"reconstruction\", fontsize=20)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.ylim([0, 500]); plt.xlim([ntime, x_max])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(dout[:500, :] - data[:500, :], aspect='auto',  origin='lower', \n",
    "           cmap='RdBu', vmax = vmax, vmin = -vmax)\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.ylim([0, 500]); plt.xlim([ntime, x_max])\n",
    "plt.title(\"residual\", fontsize=20)\n",
    "# plt.savefig(f\"../figures/tmp/shred_epo{t}.png\", bbox_inches = 'tight', dpi = 400)\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seisbench",
   "language": "python",
   "name": "seisbench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
